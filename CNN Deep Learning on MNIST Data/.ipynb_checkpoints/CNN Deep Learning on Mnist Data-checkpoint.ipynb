{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Deep Learning on the MNIST Dataset\n",
    "\n",
    "## By Christopher Hauman\n",
    "<br>\n",
    "\n",
    "This brief guide will cover building a simple Convolutional Neural Network with keras. This is a sequel to my more detailed guide and introduction to Neural Networks, [MLP Deep Learning on the MNIST Dataset](https://nbviewer.jupyter.org/github/chrisman1015/Deep-Learning/blob/master/MLP%20Deep%20Learning%20on%20MNIST%20Data/MLP%20Deep%20Learning%20on%20Mnist%20Data.ipynb). This will adapt and explain the CNN example in [keras' domumentation](https://keras.io/examples/mnist_cnn/).\n",
    "<br>\n",
    "\n",
    "If you're new to CNNs, I'd highly recommend you check out [Brandon Rohrer](https://youtu.be/FmpDIaiMIeA)'s guide on them, which will give you all the theory you need to know for this implimentation guide. This type of learning also falls under the umbrella of supervised machine learning, which you can learn much more about in my guides [here](https://github.com/chrisman1015/Supervised-Learning).\n",
    "<br>\n",
    "\n",
    "Note: This assumes you have basic knowledge of python data science basics. If you don't, or encounter something you're not familiar with, don't worry! You can get a crash course in my guide, [Cleaning MLB Statcast Data using pandas DataFrames and seaborn Visualization](https://nbviewer.jupyter.org/github/chrisman1015/Cleaning-Statcast-Data/blob/master/Cleaning%20Statcast%20Data/Cleaning%20Statcast%20Data.ipynb). \n",
    "<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'll check to make sure keras is utilizing my GPUs and we'll import the key libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:1']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get available GPUs\n",
    "from keras import backend as K\n",
    "\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure GPU is available\n",
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the data as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key difference between using keras for MLP neural networks and CNN neural networks is the input shape. MLP required the input be a flat image, while CNNs want the data to remain in the rectangular (in this case square) shape. \n",
    "<br>\n",
    "\n",
    "Let's look at the shape of the X_train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the X_training data is 60000 28x28 images. For CNN input, we specifically need the input data to be in the format (batch, height, width, channels). This means we are lacking one dimension, the channel value. Channels contains the 3 RGB values for color data, but only one for grayscale images. We can fix the shape by assigning a dimension of 1 for the channel of the X_train and X_test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train before reshaping: (60000, 28, 28)\n",
      "X_train after reshaping: (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X_train before reshaping:', X_train.shape)\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"X_train after reshaping:\", X_train.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the X_train and X_test data are in the correct shape. Let's also store the input shape which we'll pass to the first CNN layer similar to the MLP example. We'll also normalize the X data and force the y data into categorical as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get CNN first layer input shape\n",
    "input_shape = X_train[0].shape\n",
    "input_shape\n",
    "\n",
    "# normalize data\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Before we build the CNN, we'll import one of the MLP models we made in the prequel for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "MLP_model = load_model('model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MLP_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This MLP model has two Dense layers of 512 neurons each, two dropout layers (which add no parameters), and a final Dense output layer of 10 neurons. Note that this led to about 700,000 total trainable parameters. From the MLP guide, we also learned this model had an accuracy score of 0.984.\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Now let's build a comparable convolutional neural network: \n",
    "<br>\n",
    "\n",
    "We intitialize our model with the Sequential function as usual, but this time we build the first two layers with Conv2D instead of Dense.\n",
    "\n",
    "**kernel_size**<br>\n",
    "-An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.\n",
    "-A 3x3 kernal size means the convolutional window will be a 3x3 square.\n",
    "\n",
    "**Flatten** <br>\n",
    "-Flattens the output of the convolutional layer into a 2D array for Dense input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv_1 (Conv2D)              (None, 26, 26, 512)       5120      \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 26, 26, 512)       0         \n",
      "_________________________________________________________________\n",
      "Conv_2 (Conv2D)              (None, 24, 24, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 24, 24, 512)       0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 294912)            0         \n",
      "_________________________________________________________________\n",
      "Dense_output (Dense)         (None, 10)                2949130   \n",
      "=================================================================\n",
      "Total params: 5,314,058\n",
      "Trainable params: 5,314,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "\n",
    "\n",
    "model_1.add(Conv2D(512, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape, name='Conv_1'))\n",
    "\n",
    "model_1.add(Dropout(0.2, name='Dropout_1'))\n",
    "\n",
    "model_1.add(Conv2D(512, (3, 3), activation='relu', name='Conv_2'))\n",
    "\n",
    "model_1.add(Dropout(0.2, name='Dropout_2'))\n",
    "\n",
    "\n",
    "model_1.add(Flatten(name='Flatten'), )\n",
    "\n",
    "model_1.add(Dense(num_classes, activation='softmax', name=\"Dense_output\"))\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CNN model has over 7.5 times the number of trainable parameters as the MLP neural network. Let's break down the parameters in this model to see why.\n",
    "\n",
    "Conv_1: <br>\n",
    "512 kernels $\\times$ 9 parameters each  = 4608 feature maps for the first layer. We then add 512 bias terms for each kernel. <br>\n",
    "512 $\\times$ 9 + 512 = **5120 parameters**\n",
    "\n",
    "The Dropout and layers have no parameters.\n",
    "\n",
    "Conv_2: <br>\n",
    "512 kernels $\\times$  4608 feature maps + 512 bias terms = **2359808 parametes**\n",
    "\n",
    "Flatten: <br>\n",
    "We now take the 512 feature maps from Conv_2 and flatten it into one layer.\n",
    "512 $\\times$ 24 $\\times$ 24 = **294912 parameters**\n",
    "\n",
    "Dense_output: <br>\n",
    "294912 flattened pixels $\\times$ 10 output neurons + 10 bias terms = **2949130 parameters in the final layer**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that convolutional layers trigger exponential parameter growth. If we had no way to combat this, CNNs would be useless due to the extreme computational cost. Fortunately, we can use **pooling**. Pooling shrinks the size of each feature map, allowing us to use CNNs ability to detect patterns while simultaneously contolling the parameter cost.\n",
    "You can read about the pooling layer [here](http://cs231n.github.io/convolutional-networks/#pool). The argument **pool_size** is a window argument, similar to **kernel_size**.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv_1 (Conv2D)              (None, 26, 26, 512)       5120      \n",
      "_________________________________________________________________\n",
      "Pool_1 (MaxPooling2D)        (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "Conv_2 (Conv2D)              (None, 11, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "Pool_2 (MaxPooling2D)        (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "Dense_output (Dense)         (None, 10)                128010    \n",
      "=================================================================\n",
      "Total params: 2,492,938\n",
      "Trainable params: 2,492,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "\n",
    "\n",
    "model_2.add(Conv2D(512, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape, name='Conv_1'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2), name='Pool_1'))\n",
    "model_2.add(Dropout(0.2, name='Dropout_1'))\n",
    "\n",
    "model_2.add(Conv2D(512, (3, 3), activation='relu', name='Conv_2'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2), name='Pool_2'))\n",
    "model_2.add(Dropout(0.2, name='Dropout_2'))\n",
    "\n",
    "model_2.add(Flatten(name=\"Flatten\"))\n",
    "model_2.add(Dense(num_classes, activation='softmax', name=\"Dense_output\"))\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that adding these pooling layers decreased the parameters of the network by about 2/3. Let's make it smaller still, so it has the same number of total parameters as the MLP model for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv_1 (Conv2D)              (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "Pool_1 (MaxPooling2D)        (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv_2 (Conv2D)              (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "Pool_2 (MaxPooling2D)        (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "Dense_output (Dense)         (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 53,578\n",
      "Trainable params: 53,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3 = Sequential()\n",
    "\n",
    "\n",
    "model_3.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape, name='Conv_1'))\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2), name='Pool_1'))\n",
    "model_3.add(Dropout(0.2, name='Dropout_1'))\n",
    "\n",
    "model_3.add(Conv2D(64, (3, 3), activation='relu', name='Conv_2'))\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2), name='Pool_2'))\n",
    "model_3.add(Dropout(0.2, name='Dropout_2'))\n",
    "\n",
    "model_3.add(Flatten(name=\"Flatten\"))\n",
    "model_3.add(Dense(num_classes, activation='softmax', name=\"Dense_output\"))\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_3.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fit the model with an early stopping monitor as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/12\n",
      "48000/48000 [==============================] - 35s 722us/step - loss: 0.0181 - acc: 0.9936 - val_loss: 0.0118 - val_acc: 0.9966\n",
      "Epoch 2/12\n",
      "48000/48000 [==============================] - 36s 757us/step - loss: 0.0169 - acc: 0.9945 - val_loss: 0.0132 - val_acc: 0.9962\n",
      "Epoch 3/12\n",
      "48000/48000 [==============================] - 36s 747us/step - loss: 0.0149 - acc: 0.9952 - val_loss: 0.0139 - val_acc: 0.9961\n",
      "Epoch 4/12\n",
      "48000/48000 [==============================] - 35s 734us/step - loss: 0.0145 - acc: 0.9951 - val_loss: 0.0166 - val_acc: 0.9952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23799b46048>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize early stopping monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 12\n",
    "\n",
    "model_3.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks = [early_stopping_monitor],\n",
    "          validation_split=0.2,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.027888490935719164\n",
      "Test accuracy: 0.9932\n"
     ]
    }
   ],
   "source": [
    "score = model_3.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at how high the accuracy is! For image classfication, and it achieved that value on the first epoch. CNNs are an incredibly useful tool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
